{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution 1-D for RMM2 network for GPU tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Abirlal Metya, Panini Dasgupta, Manmeet Singh (16/01/2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(7)\n",
    "import random as rn\n",
    "rn.seed(7)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(10)\n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = '0'\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import hilbert_data1_jgrjd_20CRV3\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from IPython.display import clear_output\n",
    "import tqdm\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, Conv1D, Flatten,MaxPooling1D,Dropout, Activation, Flatten,Add\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train Splitter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,_,y_train = hilbert_data1_jgrjd_20CRV3.data_hilbert(datetime.datetime(1979,1,1),datetime.datetime(2008,12,31))\n",
    "x_test,_,y_test = hilbert_data1_jgrjd_20CRV3.data_hilbert(datetime.datetime(1974,6,1),datetime.datetime(1978,3,16))\n",
    "x_test2,_,y_test2 = hilbert_data1_jgrjd_20CRV3.data_hilbert(datetime.datetime(2009,1,1),datetime.datetime(2015,12,31))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test3 = hilbert_data1_jgrjd_20CRV3.data_pres(datetime.datetime(1905,1,1),datetime.datetime(2015,12,31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc3 = MinMaxScaler()\n",
    "sc5 = MinMaxScaler()\n",
    "\n",
    "sc5.fit(x_test3[:])\n",
    "\n",
    "test_x3 =  sc5.transform(x_test3[:])\n",
    "train_x = sc5.transform(x_train[:])\n",
    "test_x  = sc5.transform(x_test[:])\n",
    "test_x2  = sc5.transform(x_test2[:])\n",
    "\n",
    "\n",
    "sc3.fit(y_train[:])\n",
    "\n",
    "train_y = sc3.transform(y_train)\n",
    "test_y  = sc3.transform(y_test)\n",
    "test_y2  = sc3.transform(y_test2)\n",
    "\n",
    "#train_x.max(),test_x.max(),test_x3.max(),test_x2.max(),train_y.max(),test_y.max(),test_y2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RNN we have to choose a window. Here we choose first 120 points as predictor and next RMM value as predicted. That means RMM will be fitted using previous 120 time steps's pressure of every point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the sequence data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(window,x,*args):\n",
    "    xout  = []\n",
    "    for i in range(window,len(x)):\n",
    "        xout.append(x[i-window:i,:])\n",
    "    \n",
    "    xout = np.array(xout)\n",
    "    xout = np.reshape(xout,(xout.shape[0],xout.shape[1],xout.shape[2]))\n",
    "        \n",
    "    if np.any(len(args)):\n",
    "        for y in args:\n",
    "            yout = []\n",
    "            for i in range(window,len(y)):\n",
    "                yout.append(y[i,0])\n",
    "            yout = np.array(yout)\n",
    "            yout = yout.reshape(yout.shape[0])\n",
    "    else:\n",
    "        yout = [] \n",
    "    \n",
    "    return xout,yout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 120\n",
    "xtrain , ytrain = split_sequence(window,train_x,train_y)\n",
    "xtest , ytest   = split_sequence(window,test_x,test_y)\n",
    "xtest2 , ytest2 = split_sequence(window,test_x2,test_y2)\n",
    "xtest3,_        = split_sequence(window, test_x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cut the data according to batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_b =100 \n",
    "\n",
    "#print(x_test3.shape)\n",
    "te3_lc = ((len(x_test3)-window)//par_b)*par_b\n",
    "\n",
    "xtest3 = xtest3[:te3_lc,:,:]\n",
    "#print(xtest3.shape)\n",
    "\n",
    "#x_test3.iloc[window:window+te3_lc,:].index\n",
    "## THis perid data will be available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xtrain.shape,ytrain.shape,xtest.shape,ytest.shape)\n",
    "\n",
    "tr_lc = ((len(x_train)-window)//par_b)*par_b\n",
    "te_lc =  ((len(x_test)-window)//par_b)*par_b\n",
    "te_lc2 =  ((len(x_test2)-window)//par_b)*par_b\n",
    "\n",
    "xtrain = xtrain[:tr_lc,:,:]\n",
    "ytrain = ytrain[:tr_lc]\n",
    "xtest = xtest[:te_lc,:,:]\n",
    "ytest = ytest[:te_lc,]\n",
    "xtest2 = xtest2[:te_lc2,:,:]\n",
    "ytest2 = ytest2[:te_lc2,]\n",
    "#print(xtrain.shape,ytrain.shape,xtest.shape,xtest2.shape,ytest.shape,ytest2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Simple Convolution 1D\n",
    "* 1. Basic conv1d\n",
    "* 2. wavenet\n",
    "* 3. ENSO  paper model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf=[]\n",
    "for i in range(1,6,1):\n",
    "    nf.append([2**i,2**i,2**i])\n",
    "    nf.append([2**(i+2),2**(i+1),2**i])\n",
    "    #nf.append([2**(2*i+1),2**(i+1),2**(i)])\n",
    "    #nf.append([2**(i+1)+2**i,2**(i+1),2**(i-1)])\n",
    "\n",
    "kz=[]\n",
    "for i in range(1,4,1):\n",
    "    kz.append([2**i,2**i,2**i])\n",
    "    kz.append([2**(i+2),2**(i+1),2**(i)])\n",
    "    #kz.append([2**(i+1)+2**i,2**(i+1),2**(i)]\n",
    "    #kz.append([2**(i+1)+2**i,2**(i+1),2**(i-1)])\n",
    "\n",
    "ds = [10,20,30,40,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(params):\n",
    "    \"\"\"\n",
    "       Random number initializer is needed \n",
    "    \"\"\"\n",
    "    \n",
    "    seed(7)\n",
    "    rn.seed(7)\n",
    "    tensorflow.random.set_seed(10)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = '0'\n",
    "    \n",
    "    \n",
    "    i = params[0]; k = params[1]; j = params[2];  \n",
    "    #print('running on iteration ' + str(i)+','+str(k)+','+str(j))\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Use the Keras Conv1D function to create a 1-dimensional convolutional layer, with kernel size (filter) of 5X5 pixels and a stride of 1 in x and y directions. The Conv2D command automatically creates the activation function for you‚îÅhere we use ReLu activation.\n",
    "\n",
    "    model.add(Conv1D(nf[i][0] ,kernel_size=kz[k][0], strides=1,kernel_initializer = \"random_uniform\",\n",
    "                     bias_initializer = \"zeros\",activation='relu',\n",
    "                     input_shape=(xtrain.shape[1],xtrain.shape[2])))\n",
    "    # Then use the MaxPooling2D function to add a 2D max pooling layer, with pooling filter sized 2X2 and stride of 2 in x and y directions.\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=1, strides=1))\n",
    "\n",
    "    \n",
    "    model.add(Conv1D(nf[i][1], kernel_size=kz[k][1], strides=1,\n",
    "                     kernel_initializer = \"random_uniform\",bias_initializer = \"zeros\",activation='relu'))\n",
    "    # Then use the MaxPooling2D function to add a 2D max pooling layer, with pooling filter sized 2X2 and stride of 2 in x and y directions.\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=1, strides=1))\n",
    "\n",
    "    \n",
    "    model.add(Conv1D(nf[i][2], kernel_size=kz[k][2], strides=1,\n",
    "                     kernel_initializer = \"random_uniform\",bias_initializer = \"zeros\",activation='relu'))\n",
    "\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ds[j], kernel_initializer = \"random_uniform\",\n",
    "                    bias_initializer = \"zeros\",activation='relu'))\n",
    "\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "    model.compile(loss='mae', optimizer=opt)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0,min_delta=.01,patience=30)\n",
    "\n",
    "    \n",
    "    model.fit(xtrain, ytrain, validation_data=(xtest, ytest),batch_size=100,callbacks=[es],shuffle= False, epochs= 200,verbose=0)\n",
    "    \n",
    "    train_corr_= np.nan\n",
    "    test1_corr_ = np.nan\n",
    "    test2_corr_ = np.nan\n",
    "    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    \n",
    "    predict1   = model.predict(xtrain)\n",
    "    yy_train   = sc3.inverse_transform(predict1)\n",
    "    yy_train   = yy_train/yy_train.std()\n",
    "    train_corr_ = np.corrcoef(yy_train[:,0],ytrain)[0,1]\n",
    "    \n",
    "\n",
    "    predict1  = model.predict(xtest2)\n",
    "    yy_test1   = sc3.inverse_transform(predict1)\n",
    "    yy_test1   = yy_test1/yy_test1.std()\n",
    "    test1_corr_ = np.corrcoef(yy_test1[:,0],ytest2)[0,1]\n",
    "\n",
    "    predict2  = model.predict(xtest)\n",
    "    yy_test2   = sc3.inverse_transform(predict2)\n",
    "    yy_test2   = yy_test2/yy_test2.std()\n",
    "    test2_corr_ = np.corrcoef(yy_test2[:,0],ytest)[0,1]\n",
    "    \n",
    "    print('running on iteration ' + str(i)+','+str(k)+','+str(j))\n",
    "    print(train_corr_,test1_corr_,test2_corr_)\n",
    "    \n",
    "    return i,k,j,train_corr_,test1_corr_,test2_corr_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr = np.zeros((len(nf),len(kz),len(ds)));train_corr.fill(-1)\n",
    "test1_corr = np.zeros((len(nf),len(kz),len(ds)));test1_corr.fill(-1)\n",
    "test2_corr = np.zeros((len(nf),len(kz),len(ds)));test2_corr.fill(-1)\n",
    "\n",
    "f = open('/home/cccr/supriyo/panini/filtered_data/historical/JGRJD/conv1d/RMM1_models.txt', 'w')\n",
    "\n",
    "ii = range(len(nf))\n",
    "kk = range(len(kz))\n",
    "jj = range(len(ds))\n",
    "          \n",
    "paramlist = list(itertools.product(ii,kk,jj))\n",
    "print(multiprocessing.cpu_count())\n",
    "        \n",
    "pool = multiprocessing.Pool(processes = 2 )\n",
    "\n",
    "for i,k,j,train_corr_,test1_corr_,test2_corr_  in tqdm.tqdm(pool.imap_unordered(models, paramlist), total=len(paramlist)):\n",
    "    train_corr[i,k,j] = train_corr_\n",
    "    test1_corr[i,k,j] = test1_corr_\n",
    "    test2_corr[i,k,j] = test2_corr_ \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_rmm2_login.npy',train_corr)\n",
    "np.save('test1_rmm2_login.npy',test1_corr)\n",
    "np.save('test2_rmm2_login.npy',test2_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
