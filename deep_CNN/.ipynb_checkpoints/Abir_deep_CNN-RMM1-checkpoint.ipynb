{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cccr/supriyo/panini/filtered_data/historical/JGRJD/deep_CNN\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import hilbert_data1_jgrjd_20CRV3\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = xr.open_dataset('trop_sfc.ano_1905-2015_filtered_rm_mean120.2deg.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:   (lat: 30, lon: 180, time: 40422)\n",
       "Coordinates:\n",
       "  * time      (time) datetime64[ns] 1905-05-01 1905-05-02 ... 2015-12-31\n",
       "  * lon       (lon) float64 0.0 2.0 4.0 6.0 8.0 ... 352.0 354.0 356.0 358.0\n",
       "  * lat       (lat) float64 -29.0 -27.0 -25.0 -23.0 ... 23.0 25.0 27.0 29.0\n",
       "Data variables:\n",
       "    pres_ano  (time, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    CDI:          Climate Data Interface version 1.9.7.1 (http://mpimet.mpg.d...\n",
       "    Conventions:  CF-1.6\n",
       "    history:      Mon Jan 20 23:20:30 2020: cdo -O -sellonlatbox,0,360,-30,30...\n",
       "    CDO:          Climate Data Operators version 1.9.7.1 (http://mpimet.mpg.d...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:   (lat: 30, lon: 180, time: 40422)\n",
       "Coordinates:\n",
       "  * time      (time) datetime64[ns] 1905-05-01 1905-05-02 ... 2015-12-31\n",
       "  * lon       (lon) float64 0.0 2.0 4.0 6.0 8.0 ... 352.0 354.0 356.0 358.0\n",
       "  * lat       (lat) float64 -29.0 -27.0 -25.0 -23.0 ... 23.0 25.0 27.0 29.0\n",
       "Data variables:\n",
       "    pres_ano  (time, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    CDI:          Climate Data Interface version 1.9.7.1 (http://mpimet.mpg.d...\n",
       "    Conventions:  CF-1.6\n",
       "    history:      Mon Jan 20 23:20:30 2020: cdo -O -sellonlatbox,0,360,-30,30...\n",
       "    CDO:          Climate Data Operators version 1.9.7.1 (http://mpimet.mpg.d..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,y_train,_ = hilbert_data1_jgrjd_20CRV3.data_hilbert(datetime.datetime(1979,1,1),datetime.datetime(2008,12,31))\n",
    "_,y_test,_ = hilbert_data1_jgrjd_20CRV3.data_hilbert(datetime.datetime(1974,6,1),datetime.datetime(1978,3,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.index[0],y_train.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sel(time=slice(str(y_train.index[0]),str(y_train.index[-1])),lat=slice(-25,25))\n",
    "df_test = df.sel(time=slice(str(y_test.index[0]),str(y_test.index[-1])),lat=slice(-25,25))\n",
    "df_test3 = df.sel(lat=slice(-25,25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = df_train.pres_ano.values[:,:,:]\n",
    "xtest  = df_test.pres_ano.values[:,:,:]\n",
    "xtest3 = df_test3.pres_ano.values[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = xtrain.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ff,bins=35)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc3 = MinMaxScaler()\n",
    "sc5 = MinMaxScaler()\n",
    "\n",
    "#sc5.fit(xtest[:])\n",
    "\n",
    "#test_x3 =  sc5.transform(x_test3[:])\n",
    "#train_x = sc5.transform(xtrain[:])\n",
    "#test_x  = sc5.transform(xtest[:])\n",
    "\n",
    "\n",
    "sc3.fit(y_train[:])\n",
    "\n",
    "train_y = sc3.transform(y_train)\n",
    "test_y  = sc3.transform(y_test)\n",
    "\n",
    "#train_x.max(),test_x.max(),test_x3.max(),train_y.max(),test_y.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scalers = {}\n",
    "for i in range(xtrain.shape[1]):\n",
    "    scalers[i] = MinMaxScaler()\n",
    "    xtrain[:, i, :] = scalers[i].fit_transform(xtrain[:, i, :]) \n",
    "\n",
    "for i in range(xtest.shape[1]):\n",
    "    xtest[:, i, :] = scalers[i].transform(xtest[:, i, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain[:,:,:,None]\n",
    "xtest  = xtest[:,:,:,None]\n",
    "xtest3 = xtest3[:,:,:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten,MaxPooling2D,AveragePooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=15, kernel_size=(3, 3), activation='relu', input_shape=(xtrain.shape[1],xtrain.shape[2],1)))\n",
    "model.add(AveragePooling2D())\n",
    "\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=120, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=84, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=1, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten,MaxPooling2D\n",
    "\n",
    "model = Sequential()#add model layers\n",
    "model.add(Conv2D(156, kernel_size=3, activation='relu',input_shape=(xtrain.shape[1],xtrain.shape[2],1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(4, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xtrain.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "history = model.fit(xtrain, y_train.values, validation_data=(xtest, y_test.values), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "#num_classes = 10\n",
    "epochs = 30\n",
    "data_augmentation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=xtrain.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='mae',optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain1 = xtrain.astype('float32')\n",
    "xtest1 = xtest.astype('float32')\n",
    "xtest3 = xtest3.astype('float32')\n",
    "\n",
    "ytrain1 = y_train.values.astype('float32')\n",
    "ytest1  = y_test.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(xtrain1, ytrain1,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(xtest1, ytest1),\n",
    "              shuffle=False)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(xtrain1)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(xtrain1, ytrain1,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(xtest1, ytest1),\n",
    "                        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.history.keys()\n",
    "\n",
    "plt.plot(model.history.history['loss'],'-*',label ='train')\n",
    "plt.plot(model.history.history['val_loss'],'-*',label ='test')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('No of Epochs')\n",
    "plt.legend()\n",
    "# plt.savefig('RMM2_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_test = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yy_test,y_test,'+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_test = model.predict(xtest)\n",
    "test_corr = np.corrcoef(yy_test[:,0],y_test.values[:,0])[0,1]\n",
    "yy_test   = yy_test[:,0]/yy_test[:,0].std()\n",
    "\n",
    "# print(\"test shape = \");print(predict2.shape)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (12,4))\n",
    "\n",
    "ax[0].plot(yy_test,y_test.values[:,0],'.')\n",
    "\n",
    "ax[0].set_title('corr = %f'%test_corr)\n",
    "\n",
    "bins = np.linspace(-4.0,4.0,41)\n",
    "\n",
    "ax[1].hist(y_test.values[:,0],bins,density=True,alpha=0.5,label='observed')\n",
    "ax[1].hist(yy_test,bins,density=True,alpha=0.5,label='modeled');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_train = model.predict(xtrain)\n",
    "train_corr = np.corrcoef(yy_train[:,0],y_train.values[:,0])[0,1]\n",
    "yy_train   = yy_train[:,0]/yy_train[:,0].std()\n",
    "\n",
    "# print(\"train shape = \");print(predict2.shape)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (12,4))\n",
    "\n",
    "ax[0].plot(yy_train,y_train.values[:,0],'.')\n",
    "\n",
    "ax[0].set_title('corr = %f'%train_corr)\n",
    "\n",
    "bins = np.linspace(-4.0,4.0,41)\n",
    "\n",
    "ax[1].hist(y_train.values[:,0],bins,density=True,alpha=0.5,label='observed')\n",
    "ax[1].hist(yy_train,bins,density=True,alpha=0.5,label='modeled');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmm1 = model.predict(xtest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmm1 = pd.DataFrame(rmm1,index=pd.to_datetime(df_test3.time[:].values),columns=['rmm1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rmm1.rmm1,bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmm2.to_csv('rmm2_conv_1905_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
