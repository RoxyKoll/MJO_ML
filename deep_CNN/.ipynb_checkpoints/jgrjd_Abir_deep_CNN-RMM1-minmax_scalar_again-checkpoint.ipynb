{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cccr/supriyo/panini/filtered_data/historical/JGRJD/deep_CNN\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import hilbert_data1_jgrjd_20CRV3\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_scipy_butter(signal1,wn,lt):\n",
    "    from scipy import signal\n",
    "    w = 2/lt # Normalize the frequency\n",
    "    b, a = signal.butter(wn, w, 'low')\n",
    "    lowpass_signal = signal.filtfilt(b, a, signal1)\n",
    "\n",
    "    return lowpass_signal\n",
    "\n",
    "\n",
    "def data_y(st,en):\n",
    "\n",
    "    df2       = pd.read_csv('full_data_nn_20CR_V3.txt',index_col='date')\n",
    "    df2.index = pd.to_datetime(df2.index)\n",
    "    df2=df2[(df2.index>=st) & (df2.index<=en)]    \n",
    "    \n",
    "    ################ RMM1 #########################\n",
    "    y = df2.iloc[:,12:13]\n",
    "    lf = 10;wn = 3\n",
    "#    ### 10 days lowpass #############\n",
    "    y1 = y.copy()\n",
    "    for i in range(y.shape[1]):\n",
    "        signal = y.iloc[:,i].values\n",
    "        temp = lowpass_scipy_butter(signal,wn,lf)\n",
    "        y1.iloc[:,i] = np.real(temp)\n",
    "        \n",
    "    RMM1 = y1.iloc[119:]\n",
    "    del y,y1 \n",
    "\n",
    "    ###################################    \n",
    "    \n",
    "     ################ RMM2 #########################\n",
    "    y = df2.iloc[:,13:14]\n",
    "\n",
    "#     ### 10 days lowpass #############\n",
    "    y1 = y.copy()\n",
    "    for i in range(y.shape[1]):\n",
    "        signal = y.iloc[:,i].values\n",
    "        temp = lowpass_scipy_butter(signal,wn,lf)\n",
    "        y1.iloc[:,i] = np.real(temp)\n",
    "    RMM2 = y1.iloc[119:]\n",
    "    del y,y1 \n",
    "#     ###################################    \n",
    "       \n",
    "\n",
    "    return RMM1,RMM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = xr.open_dataset('/home/cccr/roxy/panini/JGRJD_PHD_PART_I/JGRJD_DESKTOP/MJO_reconstruct_jgrjd/20CR_V3/CNN_maps_20CRV3/trop_sfc.ano_1905-2015_filtered_rm_mean120.4deg.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,_ = data_y(datetime.datetime(1979,1,1),datetime.datetime(2008,12,31))\n",
    "y_test,_ = data_y(datetime.datetime(1974,6,1),datetime.datetime(1978,3,16))\n",
    "y_test2,_ = data_y(datetime.datetime(2009,1,1),datetime.datetime(2015,12,31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc3 = MinMaxScaler()\n",
    "\n",
    "sc3.fit(y_train[:])\n",
    "\n",
    "train_y = sc3.transform(y_train)\n",
    "test_y  = sc3.transform(y_test)\n",
    "test_y2  = sc3.transform(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test3 = df.sel(lat=slice(-25,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "xtest3 = df_test3.pres_ano.values[:,:,:].copy()\n",
    "scalers = {}\n",
    "for i in range(xtest3.shape[0]):\n",
    "    scalers = MinMaxScaler()\n",
    "    num= df_test3.pres_ano.shape[1]*df_test3.pres_ano.shape[2]\n",
    "    ff = xtest3[i,:,:].reshape(num,1)\n",
    "    scalers.fit(ff)\n",
    "    xtest3[i,:,:] = np.reshape(scalers.fit_transform(ff),(df_test3.pres_ano.shape[1],df_test3.pres_ano.shape[2]))\n",
    "\n",
    "df_test3.pres_ano.values = xtest3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_test3.sel(time=slice(str(y_train.index[0]),str(y_train.index[-1])),lat=slice(-25,25))\n",
    "df_test = df_test3.sel(time=slice(str(y_test.index[0]),str(y_test.index[-1])),lat=slice(-25,25))\n",
    "df_test2 = df_test3.sel(time=slice(str(y_test2.index[0]),str(y_test2.index[-1])),lat=slice(-25,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(xtest3.flatten(),bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = df_train.pres_ano.values[:,:,:,None]\n",
    "xtest  = df_test.pres_ano.values[:,:,:,None]\n",
    "xtest2  = df_test2.pres_ano.values[:,:,:,None]\n",
    "xtest3 = df_test3.pres_ano.values[:,:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain.shape,xtest.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "#num_classes = 10\n",
    "epochs = 60\n",
    "data_augmentation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64*2, (2, 2), padding='same',\n",
    "                 input_shape=xtrain.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64*2, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64*2, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64*2, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64*2, (2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64*2, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='mae',optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain1 = xtrain *1 #xtrain.astype('float32')\n",
    "xtest1 = xtest *1 #xtest.astype('float32')\n",
    "xtest2 = xtest2*1 #.astype('float32')\n",
    "\n",
    "xtest3 = xtest3*1 #.astype('float32')\n",
    "\n",
    "ytrain1 = train_y *1#y_train.values.astype('float32')\n",
    "ytest1  = test_y *1 #y_test.values.astype('float32')\n",
    "ytest2  = test_y2*1  #y_test2.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(xtrain1, ytrain1,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(xtest1, ytest1),\n",
    "              shuffle=False)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(xtrain1)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0,patience=10)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(xtrain1, ytrain1,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(xtest1, ytest1),callbacks=[es],\n",
    "                        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.history.keys()\n",
    "\n",
    "plt.plot(model.history.history['loss'],'-*',label ='train')\n",
    "plt.plot(model.history.history['val_loss'],'-*',label ='test')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('No of Epochs')\n",
    "plt.legend()\n",
    "# plt.savefig('RMM2_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_test = model.predict(xtest)\n",
    "test_corr = np.corrcoef(yy_test[:,0],ytest1.values[:,0])[0,1]\n",
    "yy_test   = yy_test[:,0]/yy_test[:,0].std()\n",
    "\n",
    "# print(\"test shape = \");print(predict2.shape)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (12,4))\n",
    "\n",
    "ax[0].plot(yy_test,y_test.values[:,0],'.')\n",
    "\n",
    "ax[0].set_title('corr = %f'%test_corr)\n",
    "\n",
    "bins = np.linspace(-4.0,4.0,41)\n",
    "\n",
    "ax[1].hist(y_test.values[:,0],bins,density=True,alpha=0.5,label='observed')\n",
    "ax[1].hist(yy_test,bins,density=True,alpha=0.5,label='modeled');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_test2 = model.predict(xtest2)\n",
    "test_corr2 = np.corrcoef(yy_test2[:,0],ytest2.values[:,0])[0,1]\n",
    "yy_test2   = yy_test2[:,0]/yy_test2[:,0].std()\n",
    "\n",
    "# print(\"test shape = \");print(predict2.shape)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (12,4))\n",
    "\n",
    "ax[0].plot(yy_test2,y_test2.values[:,0],'.')\n",
    "\n",
    "ax[0].set_title('corr = %f'%test_corr2)\n",
    "\n",
    "bins = np.linspace(-4.0,4.0,41)\n",
    "\n",
    "ax[1].hist(y_test2.values[:,0],bins,density=True,alpha=0.5,label='observed')\n",
    "ax[1].hist(yy_test2,bins,density=True,alpha=0.5,label='modeled');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_train = model.predict(xtrain)\n",
    "train_corr = np.corrcoef(yy_train[:,0],y_train.values[:,0])[0,1]\n",
    "yy_train   = yy_train[:,0]/yy_train[:,0].std()\n",
    "\n",
    "# print(\"train shape = \");print(predict2.shape)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize = (12,4))\n",
    "\n",
    "ax[0].plot(yy_train,y_train.values[:,0],'.')\n",
    "\n",
    "ax[0].set_title('corr = %f'%train_corr)\n",
    "\n",
    "bins = np.linspace(-4.0,4.0,41)\n",
    "\n",
    "ax[1].hist(y_train.values[:,0],bins,density=True,alpha=0.5,label='observed')\n",
    "ax[1].hist(yy_train,bins,density=True,alpha=0.5,label='modeled');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmm2 = model.predict(xtest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmm2 = pd.DataFrame(rmm2,index=pd.to_datetime(df_test3.time[:].values),columns=['rmm2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rmm2.rmm2,bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmm2.to_csv('deep_CNN_rmm2_minmaxscalar_batchsize_100_woymnmx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rmm2.rmm2)\n",
    "plt.axhline(3)\n",
    "plt.axhline(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"deep_CNN_rmm2_minmaxscalar_batchsize_100_woymnmx.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
